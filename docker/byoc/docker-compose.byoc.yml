version: '3.8'

# BYOC (Bring Your Own Container) Configuration for Livepeer Orchestrators
# This allows orchestrators to bring their own specialized containers

services:
  # BYOC GPU Transcoder - Custom GPU-accelerated transcoding container
  byoc-gpu-transcoder:
    image: ${BYOC_GPU_IMAGE:-nvidia/cuda:11.8.0-runtime-ubuntu22.04}
    container_name: byoc-gpu-transcoder
    hostname: byoc-gpu-transcoder
    networks:
      - orchestrator-network
    environment:
      - ORCHESTRATOR_ID=${ORCHESTRATOR_ID:-orch-001}
      - MANAGER_URL=${MANAGER_URL:-http://central-manager:8010}
      - CAPABILITY=gpu_transcoding
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ./transcoding:/workspace
      - /tmp/.X11-unix:/tmp/.X11-unix:rw
    command: >
      sh -c "echo 'BYOC GPU Transcoder Started' && 
             while true; do 
               echo 'Processing GPU tasks...'; 
               sleep 30; 
             done"
    restart: unless-stopped
    labels:
      - "orchestrator.type=byoc"
      - "orchestrator.capability=gpu_transcoding"

  # BYOC Payment Processor - Custom payment processing container
  byoc-payment-processor:
    image: ${BYOC_PAYMENT_IMAGE:-python:3.11-slim}
    container_name: byoc-payment-processor
    hostname: byoc-payment-processor
    networks:
      - orchestrator-network
    environment:
      - ORCHESTRATOR_ID=${ORCHESTRATOR_ID:-orch-001}
      - MANAGER_URL=${MANAGER_URL:-http://central-manager:8010}
      - CAPABILITY=payment_processing
      - ETH_RPC_URL=${ETH_RPC_URL:-https://mainnet.infura.io/v3/YOUR_KEY}
      - PAYMENT_CONTRACT_ADDRESS=${PAYMENT_CONTRACT_ADDRESS}
    volumes:
      - ./payment-data:/data
      - ./scripts:/scripts:ro
    command: >
      sh -c "pip install web3 httpx fastapi uvicorn && 
             echo 'BYOC Payment Processor Started' && 
             python -c '
      import asyncio
      import httpx
      import json
      import os
      from datetime import datetime
      
      async def register_with_manager():
          manager_url = os.getenv(\"MANAGER_URL\", \"http://central-manager:8010\")
          orchestrator_id = os.getenv(\"ORCHESTRATOR_ID\", \"orch-001\")
          
          async with httpx.AsyncClient() as client:
              try:
                  response = await client.post(
                      f\"{manager_url}/api/v1/livepeer/orchestrators/register\",
                      json={
                          \"orchestrator_id\": f\"{orchestrator_id}-payment\",
                          \"endpoint\": \"http://byoc-payment-processor:8000\",
                          \"capabilities\": [\"payment_processing\", \"ethereum\"]
                      }
                  )
                  print(f\"Registration response: {response.status_code}\")
              except Exception as e:
                  print(f\"Registration failed: {e}\")
      
      async def main():
          await register_with_manager()
          while True:
              print(f\"Processing payments at {datetime.now()}\")
              await asyncio.sleep(30)
      
      asyncio.run(main())
      '"
    restart: unless-stopped
    labels:
      - "orchestrator.type=byoc"
      - "orchestrator.capability=payment_processing"

  # BYOC Stream Processor - Custom streaming container
  byoc-stream-processor:
    image: ${BYOC_STREAM_IMAGE:-jrottenberg/ffmpeg:4.4-alpine}
    container_name: byoc-stream-processor
    hostname: byoc-stream-processor
    networks:
      - orchestrator-network
    environment:
      - ORCHESTRATOR_ID=${ORCHESTRATOR_ID:-orch-001}
      - MANAGER_URL=${MANAGER_URL:-http://central-manager:8010}
      - CAPABILITY=stream_processing
      - RTMP_URL=${RTMP_URL:-rtmp://localhost:1935/live}
      - HLS_OUTPUT=/var/www/hls
    volumes:
      - ./streams:/streams
      - ./hls-output:/var/www/hls
    command: >
      sh -c "echo 'BYOC Stream Processor Started' && 
             while true; do 
               echo 'Processing streams...'; 
               sleep 30; 
             done"
    restart: unless-stopped
    labels:
      - "orchestrator.type=byoc"
      - "orchestrator.capability=stream_processing"

  # BYOC AI Model Server - Custom ML inference container
  byoc-ai-model:
    image: ${BYOC_AI_IMAGE:-pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime}
    container_name: byoc-ai-model
    hostname: byoc-ai-model
    networks:
      - orchestrator-network
    environment:
      - ORCHESTRATOR_ID=${ORCHESTRATOR_ID:-orch-001}
      - MANAGER_URL=${MANAGER_URL:-http://central-manager:8010}
      - CAPABILITY=ai_inference
      - MODEL_PATH=/models
      - INFERENCE_BATCH_SIZE=32
    volumes:
      - ./models:/models
      - ./inference-cache:/cache
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: >
      sh -c "pip install fastapi uvicorn torch torchvision && 
             echo 'BYOC AI Model Server Started' && 
             python -c '
      import torch
      print(f\"PyTorch version: {torch.__version__}\")
      print(f\"CUDA available: {torch.cuda.is_available()}\")
      if torch.cuda.is_available():
          print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")
      
      import time
      while True:
          print(\"AI Model Server running...\")
          time.sleep(30)
      '"
    restart: unless-stopped
    labels:
      - "orchestrator.type=byoc"
      - "orchestrator.capability=ai_inference"

  # BYOC Custom Analytics - User-provided analytics container
  byoc-analytics:
    image: ${BYOC_ANALYTICS_IMAGE:-grafana/grafana:latest}
    container_name: byoc-analytics
    hostname: byoc-analytics
    networks:
      - orchestrator-network
    environment:
      - ORCHESTRATOR_ID=${ORCHESTRATOR_ID:-orch-001}
      - MANAGER_URL=${MANAGER_URL:-http://central-manager:8010}
      - CAPABILITY=analytics
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_INSTALL_PLUGINS=redis-datasource
    volumes:
      - byoc-analytics-data:/var/lib/grafana
      - ./grafana-dashboards:/etc/grafana/provisioning/dashboards
    ports:
      - "3100:3000"  # Different port to avoid conflicts
    restart: unless-stopped
    labels:
      - "orchestrator.type=byoc"
      - "orchestrator.capability=analytics"

  # BYOC Registry - Tracks all BYOC containers
  byoc-registry:
    image: redis:7-alpine
    container_name: byoc-registry
    hostname: byoc-registry
    networks:
      - orchestrator-network
    volumes:
      - byoc-registry-data:/data
    ports:
      - "6390:6379"  # Different port for BYOC registry
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    restart: unless-stopped
    labels:
      - "orchestrator.type=byoc"
      - "orchestrator.capability=registry"

networks:
  orchestrator-network:
    name: orchestrator-network
    external: true

volumes:
  byoc-analytics-data:
  byoc-registry-data: